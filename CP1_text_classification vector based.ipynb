{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Text Classification\n",
    "This code trains a HT model using SVM.SVC or Multinomial Naive Bayes.\n",
    "This code convert each document into its vector and only sotres vectors into memory for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from scipy import sparse\n",
    "from random import shuffle\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "#89668 negative docs \n",
    "#203181 positive docs\n",
    "count = 90000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find list of unique words from both negative and positive documents\n",
    "To find unique words for each document, I used the word return from vectorizing, this methos may not be very efficient as we vectorize documents again in next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 negative processed (finding unique word)\n",
      "40000 negative processed (finding unique word)\n",
      "60000 negative processed (finding unique word)\n",
      "80000 negative processed (finding unique word)\n",
      "20000 positive processed (finding unique word)\n",
      "40000 positive processed (finding unique word)\n",
      "60000 positive processed (finding unique word)\n",
      "80000 positive processed (finding unique word)\n"
     ]
    }
   ],
   "source": [
    "unique_words = []\n",
    "\n",
    "def find_unique_word(words):\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "\n",
    "def get_vector_words(text):\n",
    "    vector = vectorizer.fit_transform([text])\n",
    "    words_or_features = vectorizer.get_feature_names()\n",
    "    return vector.toarray()[0], words_or_features\n",
    "\n",
    "with open(\"cp1_negative_train_UPDATED.json\", 'r') as f:\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        if c < count:\n",
    "            extracted_text = json.loads(line)['extracted_text']\n",
    "            vector, words = get_vector_words(extracted_text)\n",
    "            find_unique_word(words)\n",
    "            c += 1\n",
    "            if c%20000 == 0:\n",
    "                print \"{0} negative processed (finding unique word)\".format(c)\n",
    "    f.close()\n",
    "\n",
    "with open(\"CP1_train_ads.json\", 'r') as f:\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        if c < count:\n",
    "            extracted_text = json.loads(line)['extracted_text']\n",
    "            vector, words = get_vector_words(extracted_text)\n",
    "            find_unique_word(words)\n",
    "            c += 1\n",
    "            if c%20000 == 0:\n",
    "                print \"{0} positive processed (finding unique word)\".format(c)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize documents into same vector shape\n",
    "Convert each document to vector with same shape ([num_unique]) and store with their lables (HT or not_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 negative processed (creating vector)\n",
      "40000 negative processed (creating vector)\n",
      "60000 negative processed (creating vector)\n",
      "80000 negative processed (creating vector)\n",
      "20000 postitve processed (creating vector)\n",
      "40000 postitve processed (creating vector)\n",
      "60000 postitve processed (creating vector)\n",
      "80000 postitve processed (creating vector)\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "num_unique = len(unique_words)\n",
    "\n",
    "def add_vector_to_data(vector, words, num_unique, label):\n",
    "    tmp = np.zeros(num_unique)\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            index = unique_words.index(word.encode())\n",
    "            tmp[index] = vector[i]\n",
    "        except:\n",
    "            pass\n",
    "    all_data.append([sparse.csr_matrix(tmp), label])\n",
    "\n",
    "with open(\"cp1_negative_train_UPDATED.json\", 'r') as f:\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        if c < count:\n",
    "            extracted_text = json.loads(line)['extracted_text']\n",
    "            vector, words = get_vector_words(extracted_text)\n",
    "            add_vector_to_data(vector, words, num_unique, 'not_ht')\n",
    "            c += 1\n",
    "            if c%20000 == 0:\n",
    "                print \"{0} negative processed (creating vector)\".format(c)\n",
    "    f.close()\n",
    "\n",
    "with open(\"CP1_train_ads.json\", 'r') as f:\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        if c < count:\n",
    "            extracted_text = json.loads(line)['extracted_text']\n",
    "            vector, words = get_vector_words(extracted_text)\n",
    "            add_vector_to_data(vector, words, num_unique, 'ht')\n",
    "            c += 1\n",
    "            if c%20000 == 0:\n",
    "                print \"{0} postitve processed (creating vector)\".format(c)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary unique words and shuffle vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del unique_words\n",
    "shuffle(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate vectors and labales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "labels = []\n",
    "for each in all_data:\n",
    "    vectors.append(each[0])\n",
    "    labels.append(each[1])\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate training and test data in ratio of 80 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_labels = len(labels)\n",
    "len_vectors = len(vectors)\n",
    "if len_labels == len_vectors:\n",
    "    train_num = int((80*len_labels)/100)\n",
    "    test_num = len_labels - train_num\n",
    "train_data = vectors[:train_num]\n",
    "test_data = vectors[-1*(test_num):]\n",
    "train_labels = labels[:train_num]\n",
    "test_labels = labels[-1*(test_num):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack all training and test vectors together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vectors = vstack(train_data)\n",
    "test_vectors = vstack(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train model using either SVM or NB\n",
    "You can switch between SVC() or MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Calculate confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15810  2199]\n",
      " [ 1847 16078]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(test_labels, predicted)\n",
    "print confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886558627264\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(test_labels, predicted, pos_label=\"ht\")\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store trained model into python pickel for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump( clf, open( \"ht_clf.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
